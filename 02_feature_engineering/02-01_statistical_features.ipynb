{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-entrance",
   "metadata": {
    "papermill": {
     "duration": 0.008973,
     "end_time": "2021-05-20T15:46:10.240981",
     "exception": false,
     "start_time": "2021-05-20T15:46:10.232008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "This notebook is used for creating more than 40 manual statistical features for each dataset.   \n",
    "The original dataset is too large so that the full running takes a long time.   \n",
    "Only an example running (for the first rows) is shown here.  \n",
    "The same procedures are repeated for training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crude-blend",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T15:46:10.262614Z",
     "iopub.status.busy": "2021-05-20T15:46:10.261970Z",
     "iopub.status.idle": "2021-05-20T15:46:11.998847Z",
     "shell.execute_reply": "2021-05-20T15:46:11.997667Z",
     "shell.execute_reply.started": "2021-05-20T15:45:14.886410Z"
    },
    "papermill": {
     "duration": 1.749775,
     "end_time": "2021-05-20T15:46:11.999012",
     "exception": false,
     "start_time": "2021-05-20T15:46:10.249237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chubby-hamilton",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T15:46:12.025772Z",
     "iopub.status.busy": "2021-05-20T15:46:12.025045Z",
     "iopub.status.idle": "2021-05-20T15:46:20.655405Z",
     "shell.execute_reply": "2021-05-20T15:46:20.654703Z",
     "shell.execute_reply.started": "2021-05-20T15:45:16.877118Z"
    },
    "papermill": {
     "duration": 8.64812,
     "end_time": "2021-05-20T15:46:20.655546",
     "exception": false,
     "start_time": "2021-05-20T15:46:12.007426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = '../input/bt5153-applied-machine-learning-2021-spring/train.csv'\n",
    "#file = '../input/bt5153-applied-machine-learning-2021-spring/test.csv'\n",
    "#file = '../input/bn-vect-manual-out95/test_over95.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "EXAMPLE_RUN = True\n",
    "if EXAMPLE_RUN:\n",
    "    df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "female-mountain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T15:46:20.677719Z",
     "iopub.status.busy": "2021-05-20T15:46:20.676974Z",
     "iopub.status.idle": "2021-05-20T15:46:20.679795Z",
     "shell.execute_reply": "2021-05-20T15:46:20.679171Z",
     "shell.execute_reply.started": "2021-05-20T15:45:24.824295Z"
    },
    "papermill": {
     "duration": 0.016102,
     "end_time": "2021-05-20T15:46:20.679926",
     "exception": false,
     "start_time": "2021-05-20T15:46:20.663824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#basic data pre-processing functions before feature engineering\n",
    "def remove_xDxAs(text):\n",
    "    return text.replace('&#xD;&#xA;', ' ')\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    # Function to remove whitespace\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def data_preprocess(text):\n",
    "    # Function to pre-process text\n",
    "    text = remove_xDxAs(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-snapshot",
   "metadata": {
    "papermill": {
     "duration": 0.007887,
     "end_time": "2021-05-20T15:46:20.695860",
     "exception": false,
     "start_time": "2021-05-20T15:46:20.687973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "arbitrary-chambers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T15:46:20.719270Z",
     "iopub.status.busy": "2021-05-20T15:46:20.718639Z",
     "iopub.status.idle": "2021-05-20T15:46:20.722879Z",
     "shell.execute_reply": "2021-05-20T15:46:20.722322Z",
     "shell.execute_reply.started": "2021-05-20T15:45:24.833524Z"
    },
    "papermill": {
     "duration": 0.019157,
     "end_time": "2021-05-20T15:46:20.723010",
     "exception": false,
     "start_time": "2021-05-20T15:46:20.703853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check the tags of words\n",
    "pos_family = {\n",
    "    'noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "    'pron': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "    'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "    'adj': ['JJ', 'JJR', 'JJS'],\n",
    "    'adv': ['RB', 'RBR', 'RBS', 'WRB']\n",
    "    }\n",
    "\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attempted-senator",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T15:46:20.754876Z",
     "iopub.status.busy": "2021-05-20T15:46:20.753763Z",
     "iopub.status.idle": "2021-05-20T15:46:20.756951Z",
     "shell.execute_reply": "2021-05-20T15:46:20.756433Z",
     "shell.execute_reply.started": "2021-05-20T15:45:24.849463Z"
    },
    "papermill": {
     "duration": 0.025724,
     "end_time": "2021-05-20T15:46:20.757096",
     "exception": false,
     "start_time": "2021-05-20T15:46:20.731372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def if_url(text):\n",
    "    #check whether the text contains a url\n",
    "    if 'http://' in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def count_greek_char(text):\n",
    "    #count greek characters\n",
    "    cnt = 0\n",
    "    for i in text:\n",
    "        if (ord(i) >= 945) and (ord(i) < 970):\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "def count_japanese(text):\n",
    "    #count Japanese characters\n",
    "    jap = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\uAC00-\\uD7A3]') \n",
    "    cnt = 0\n",
    "    for i in range(len(text)):\n",
    "        if jap.search(text[i]):\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "def count_chinese(text):\n",
    "    #count Chinese characters\n",
    "    zhPattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "    cnt = 0\n",
    "    for i in range(len(text)):\n",
    "        if zhPattern.search(text[i]):\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def isEnglish(text):\n",
    "    #check whether all characters are English\n",
    "    return text.translate(string.punctuation).isalnum()\n",
    "\n",
    "\n",
    "def compute_ner(text):\n",
    "    #count the number of words with special meanings\n",
    "    types = ['PERSON', 'ORGANIZATION', 'LOCATION', 'DATE',\n",
    "             'TIME', 'MONEY', 'PERCENT', 'FACILITY', 'GPE']\n",
    "\n",
    "    dic = dict.fromkeys(types, 0)\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "\n",
    "    named_entities = []\n",
    "    for ne_taged_sentence in ne_chunked_sents:\n",
    "        for tagged_tree in ne_taged_sentence:\n",
    "            if hasattr(tagged_tree, 'label'):\n",
    "                name = tagged_tree[0][0]\n",
    "                type = tagged_tree.label()\n",
    "                named_entities.append((name, type))\n",
    "\n",
    "    entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "    tmp = entity_frame['Entity Type'].value_counts().reset_index().set_index('index')\n",
    "\n",
    "    for i in range(tmp.shape[0]):\n",
    "        dic[tmp.index[i]] = tmp['Entity Type'].iloc[i]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ruled-supervision",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T15:46:20.795626Z",
     "iopub.status.busy": "2021-05-20T15:46:20.792773Z",
     "iopub.status.idle": "2021-05-20T15:46:20.799686Z",
     "shell.execute_reply": "2021-05-20T15:46:20.799131Z",
     "shell.execute_reply.started": "2021-05-20T15:45:24.868571Z"
    },
    "papermill": {
     "duration": 0.034705,
     "end_time": "2021-05-20T15:46:20.799826",
     "exception": false,
     "start_time": "2021-05-20T15:46:20.765121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#the main function for creating manual statistical features\n",
    "def get_manual_features(df):\n",
    "    df['Text'] = df.Text.apply(data_preprocess)\n",
    "    df['Split'] = df.Text.apply(lambda x: x.split())\n",
    "\n",
    "    df['char_count'] = df.Text.apply(len)  \n",
    "    df['word_count'] = df.Split.apply(lambda x: len(x))  \n",
    "    df['word_density'] = df['char_count'] / (df['word_count'] + 1)  \n",
    "\n",
    "    df['punc_count'] = df.Text.apply(lambda x:\n",
    "                                  len(\"\".join(_ for _ in x if _ in string.punctuation)))  \n",
    "    df['punc_density'] = df['punc_count'] / (df['word_count'] + 1)  \n",
    "\n",
    "    df['title_count'] = df.Split.apply(lambda x:\n",
    "                                    len([wrd for wrd in x if wrd.istitle()]))  \n",
    "    df['title_density'] = df['title_count'] / (df['word_count'] + 1)  \n",
    "\n",
    "    df['upper_count'] = df.Split.apply(lambda x:\n",
    "                                    len([wrd for wrd in x if wrd.isupper()]))  \n",
    "    df['upper_density'] = df['upper_count'] / (df['word_count'] + 1)  \n",
    "\n",
    "    df['noun_count'] = df.Text.apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "    df['noun_density'] = df['noun_count'] / (df['word_count'] + 1)\n",
    "\n",
    "    df['verb_count'] = df.Text.apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "    df['verb_density'] = df['verb_count'] / (df['word_count'] + 1)\n",
    "\n",
    "    df['adj_count'] = df.Text.apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "    df['adj_density'] = df['adj_count'] / (df['word_count'] + 1)\n",
    "\n",
    "    df['adv_count'] = df.Text.apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "    df['adv_density'] = df['adv_count'] / (df['word_count'] + 1)\n",
    "\n",
    "    df['pron_count'] = df.Text.apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "    df['pron_density'] = df['pron_count'] / (df['word_count'] + 1)\n",
    "\n",
    "    df['avg_word_len'] = df.Split.apply(lambda x: np.mean([len(item) for item in x]))  \n",
    "    df['max_word_len'] = df.Split.apply(lambda x: np.max([len(item) for item in x]))  \n",
    "\n",
    "    df['num_word_count'] = df.Split.apply(lambda x: np.sum([item.isdigit() for item in x]))  \n",
    "    df['num_char_count'] = df.Text.apply(lambda x: np.sum([item.isdigit() for item in x]))  \n",
    "    df['num_word_density'] = df['num_word_count'] / df['word_count']  \n",
    "    df['num_char_density'] = df['num_char_count'] / df['word_count']  \n",
    "\n",
    "    df['alnum_count'] = df.Split.apply(lambda x: np.sum([item.isalnum() for item in x]))  \n",
    "    df['alnum_density'] = df['alnum_count'] / df['word_count']\n",
    "\n",
    "    df['alpha_count'] = df.Split.apply(lambda x: np.sum([item.isalpha() for item in x]))  \n",
    "    df['alpha_density'] = df['alpha_count'] / df['word_count']\n",
    "\n",
    "    df['has_url'] = df.Text.apply(if_url)\n",
    "\n",
    "    df['ps_count'] = df.Text.apply(lambda x: compute_ner(x)['PERSON'])\n",
    "    df['ps_density'] = df['ps_count'] / df['word_count']\n",
    "\n",
    "    df['org_count'] = df.Text.apply(lambda x: compute_ner(x)['ORGANIZATION'])\n",
    "    df['org_density'] = df['org_count'] / df['word_count']\n",
    "\n",
    "    df['gpe_count'] = df.Text.apply(lambda x: compute_ner(x)['GPE'])\n",
    "    df['gpe_density'] = df['gpe_count'] / df['word_count']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "classified-judgment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T15:46:20.836027Z",
     "iopub.status.busy": "2021-05-20T15:46:20.835174Z",
     "iopub.status.idle": "2021-05-20T15:46:34.319420Z",
     "shell.execute_reply": "2021-05-20T15:46:34.318760Z",
     "shell.execute_reply.started": "2021-05-20T15:45:24.893988Z"
    },
    "papermill": {
     "duration": 13.511853,
     "end_time": "2021-05-20T15:46:34.319560",
     "exception": false,
     "start_time": "2021-05-20T15:46:20.807707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Outcome', 'Text', 'Id', 'Split', 'char_count', 'word_count',\n",
      "       'word_density', 'punc_count', 'punc_density', 'title_count',\n",
      "       'title_density', 'upper_count', 'upper_density', 'noun_count',\n",
      "       'noun_density', 'verb_count', 'verb_density', 'adj_count',\n",
      "       'adj_density', 'adv_count', 'adv_density', 'pron_count', 'pron_density',\n",
      "       'avg_word_len', 'max_word_len', 'num_word_count', 'num_char_count',\n",
      "       'num_word_density', 'num_char_density', 'alnum_count', 'alnum_density',\n",
      "       'alpha_count', 'alpha_density', 'has_url', 'ps_count', 'ps_density',\n",
      "       'org_count', 'org_density', 'gpe_count', 'gpe_density'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Text</th>\n",
       "      <th>Id</th>\n",
       "      <th>Split</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>punc_density</th>\n",
       "      <th>title_count</th>\n",
       "      <th>...</th>\n",
       "      <th>alnum_density</th>\n",
       "      <th>alpha_count</th>\n",
       "      <th>alpha_density</th>\n",
       "      <th>has_url</th>\n",
       "      <th>ps_count</th>\n",
       "      <th>ps_density</th>\n",
       "      <th>org_count</th>\n",
       "      <th>org_density</th>\n",
       "      <th>gpe_count</th>\n",
       "      <th>gpe_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>I am having a problem with the first example o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, am, having, a, problem, with, the, first, ...</td>\n",
       "      <td>328</td>\n",
       "      <td>49</td>\n",
       "      <td>6.560000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>35</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>2</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>everyone, I met a tough definite integral as f...</td>\n",
       "      <td>2</td>\n",
       "      <td>[everyone,, I, met, a, tough, definite, integr...</td>\n",
       "      <td>240</td>\n",
       "      <td>37</td>\n",
       "      <td>6.315789</td>\n",
       "      <td>63</td>\n",
       "      <td>1.657895</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>12</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Please dont lynch me, but i've never sat throu...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Please, dont, lynch, me,, but, i've, never, s...</td>\n",
       "      <td>244</td>\n",
       "      <td>48</td>\n",
       "      <td>4.979592</td>\n",
       "      <td>11</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>36</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>How to calculate $ \\mathbb{Z}[x] /\\langle2x-1\\...</td>\n",
       "      <td>4</td>\n",
       "      <td>[How, to, calculate, $, \\mathbb{Z}[x], /\\langl...</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>12</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>When somebody rings or texts my iPhone it is n...</td>\n",
       "      <td>5</td>\n",
       "      <td>[When, somebody, rings, or, texts, my, iPhone,...</td>\n",
       "      <td>170</td>\n",
       "      <td>36</td>\n",
       "      <td>4.594595</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>35</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outcome                                               Text  Id  \\\n",
       "0       14  I am having a problem with the first example o...   1   \n",
       "1       14  everyone, I met a tough definite integral as f...   2   \n",
       "2        7  Please dont lynch me, but i've never sat throu...   3   \n",
       "3       14  How to calculate $ \\mathbb{Z}[x] /\\langle2x-1\\...   4   \n",
       "4        2  When somebody rings or texts my iPhone it is n...   5   \n",
       "\n",
       "                                               Split  char_count  word_count  \\\n",
       "0  [I, am, having, a, problem, with, the, first, ...         328          49   \n",
       "1  [everyone,, I, met, a, tough, definite, integr...         240          37   \n",
       "2  [Please, dont, lynch, me,, but, i've, never, s...         244          48   \n",
       "3  [How, to, calculate, $, \\mathbb{Z}[x], /\\langl...          55           7   \n",
       "4  [When, somebody, rings, or, texts, my, iPhone,...         170          36   \n",
       "\n",
       "   word_density  punc_count  punc_density  title_count  ...  alnum_density  \\\n",
       "0      6.560000          41      0.820000           10  ...       0.714286   \n",
       "1      6.315789          63      1.657895            2  ...       0.324324   \n",
       "2      4.979592          11      0.224490            4  ...       0.812500   \n",
       "3      6.875000          12      1.500000            1  ...       0.428571   \n",
       "4      4.594595           1      0.027027            1  ...       0.972222   \n",
       "\n",
       "   alpha_count  alpha_density  has_url  ps_count  ps_density  org_count  \\\n",
       "0           35       0.714286    False         3    0.061224          1   \n",
       "1           12       0.324324    False         0    0.000000          0   \n",
       "2           36       0.750000    False         0    0.000000          1   \n",
       "3            3       0.428571    False         0    0.000000          0   \n",
       "4           35       0.972222    False         0    0.000000          2   \n",
       "\n",
       "   org_density  gpe_count  gpe_density  \n",
       "0     0.020408          2     0.040816  \n",
       "1     0.000000          0     0.000000  \n",
       "2     0.020833          1     0.020833  \n",
       "3     0.000000          0     0.000000  \n",
       "4     0.055556          0     0.000000  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = get_manual_features(df)\n",
    "df_feat.to_csv('df_train_all_feat.csv', index=False)\n",
    "#df_feat.to_csv('df_test_feat.csv', index=False)\n",
    "#df_feat.to_csv('df_outp95_feat.csv', index=False)\n",
    "print(df_feat.columns)\n",
    "df_feat.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.071978,
   "end_time": "2021-05-20T15:46:35.139803",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-20T15:46:04.067825",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
